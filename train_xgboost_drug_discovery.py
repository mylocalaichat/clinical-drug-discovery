"""
Complete Drug-Disease Prediction Pipeline with XGBoost

This script:
1. Loads embeddings from Memgraph (generated by Node2Vec)
2. Creates training pairs (known treatments + negatives)
3. Flattens embeddings into feature vectors
4. Trains XGBoost classifier with cross-validation
5. Predicts probabilities for ALL drug-disease pairs
6. Outputs ranked predictions

Usage:
    python train_xgboost_drug_discovery.py
"""

import pandas as pd
import numpy as np
from neo4j import GraphDatabase
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc
from xgboost import XGBClassifier
import random
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class DrugDiseasePredictor:
    def __init__(self, uri='bolt://localhost:7687'):
        """Initialize connection to Memgraph"""
        self.driver = GraphDatabase.driver(uri, auth=None)
        self.session = self.driver.session()

    def close(self):
        """Close database connection"""
        self.session.close()
        self.driver.close()

    # =====================================================================
    # STEP 1: Load Embeddings from Memgraph
    # =====================================================================

    def load_embeddings(self):
        """Load node embeddings from Memgraph"""
        print("\n" + "="*80)
        print("STEP 1: Loading Embeddings from Memgraph")
        print("="*80)

        # Get all nodes with embeddings
        result = self.session.run("""
            MATCH (n)
            WHERE EXISTS(n.embedding) AND n.is_example = true
            RETURN n.node_id as id,
                   n.node_name as name,
                   n.node_type as type,
                   n.embedding as embedding
        """)

        embeddings = {}
        for record in result:
            embeddings[record['id']] = {
                'name': record['name'],
                'type': record['type'],
                'embedding': np.array(record['embedding'])
            }

        print(f"✓ Loaded embeddings for {len(embeddings)} nodes")

        # Separate by type
        drugs = {k: v for k, v in embeddings.items() if v['type'] == 'drug'}
        diseases = {k: v for k, v in embeddings.items() if v['type'] == 'disease'}

        print(f"  - Drugs: {len(drugs)}")
        print(f"  - Diseases: {len(diseases)}")
        print(f"  - Embedding dimensions: {len(list(embeddings.values())[0]['embedding'])}")

        return embeddings, drugs, diseases

    # =====================================================================
    # STEP 2: Load Known Drug-Disease Relationships
    # =====================================================================

    def load_known_relationships(self):
        """Load known drug-disease treatment relationships"""
        print("\n" + "="*80)
        print("STEP 2: Loading Known Drug-Disease Relationships")
        print("="*80)

        result = self.session.run("""
            MATCH (drug:Node)-[r:RELATES {relation: 'drug_treats_disease'}]->(disease:Node)
            WHERE drug.is_example = true AND disease.is_example = true
            RETURN drug.node_id as drug_id,
                   drug.node_name as drug_name,
                   disease.node_id as disease_id,
                   disease.node_name as disease_name
        """)

        known_pairs = []
        for record in result:
            known_pairs.append({
                'drug_id': record['drug_id'],
                'drug_name': record['drug_name'],
                'disease_id': record['disease_id'],
                'disease_name': record['disease_name']
            })

        print(f"✓ Found {len(known_pairs)} known treatment relationships")

        for i, pair in enumerate(known_pairs[:5], 1):
            print(f"  {i}. {pair['drug_name']} → {pair['disease_name']}")

        return known_pairs

    # =====================================================================
    # STEP 3: Create Training Dataset
    # =====================================================================

    def create_training_data(self, drugs, diseases, known_pairs, embeddings,
                            negative_ratio=1.0, unknown_ratio=0.5):
        """
        Create training dataset with positive, negative, and unknown samples

        Args:
            drugs: Dictionary of drug nodes
            diseases: Dictionary of disease nodes
            known_pairs: List of known drug-disease relationships
            embeddings: Dictionary of all embeddings
            negative_ratio: Ratio of negatives to positives (1.0 = equal)
            unknown_ratio: Ratio of unknowns to positives (0.5 = half as many)
        """
        print("\n" + "="*80)
        print("STEP 3: Creating Training Dataset")
        print("="*80)

        # Create set of known pairs for fast lookup
        known_set = {(p['drug_id'], p['disease_id']) for p in known_pairs}

        # Get all drug and disease IDs
        drug_ids = list(drugs.keys())
        disease_ids = list(diseases.keys())

        print(f"  Total possible pairs: {len(drug_ids)} × {len(disease_ids)} = {len(drug_ids) * len(disease_ids):,}")

        # ===== POSITIVE SAMPLES (Label = 1) =====
        print("\n  Creating positive samples...")
        positive_samples = []

        for pair in known_pairs:
            drug_id = pair['drug_id']
            disease_id = pair['disease_id']

            if drug_id in embeddings and disease_id in embeddings:
                positive_samples.append({
                    'drug_id': drug_id,
                    'disease_id': disease_id,
                    'drug_name': pair['drug_name'],
                    'disease_name': pair['disease_name'],
                    'label': 1,  # Treats
                    'type': 'positive'
                })

        print(f"  ✓ Positive samples: {len(positive_samples)}")

        # ===== NEGATIVE SAMPLES (Label = 0) =====
        print("\n  Creating negative samples...")
        negative_samples = []
        n_negatives = int(len(positive_samples) * negative_ratio)

        attempts = 0
        max_attempts = n_negatives * 10

        while len(negative_samples) < n_negatives and attempts < max_attempts:
            drug_id = random.choice(drug_ids)
            disease_id = random.choice(disease_ids)

            # Only include if NOT a known relationship
            if (drug_id, disease_id) not in known_set:
                negative_samples.append({
                    'drug_id': drug_id,
                    'disease_id': disease_id,
                    'drug_name': embeddings[drug_id]['name'],
                    'disease_name': embeddings[disease_id]['name'],
                    'label': 0,  # Does not treat
                    'type': 'negative'
                })

            attempts += 1

        print(f"  ✓ Negative samples: {len(negative_samples)}")

        # ===== UNKNOWN SAMPLES (Label = 2) =====
        print("\n  Creating unknown samples...")
        unknown_samples = []
        n_unknowns = int(len(positive_samples) * unknown_ratio)

        attempts = 0
        max_attempts = n_unknowns * 10

        while len(unknown_samples) < n_unknowns and attempts < max_attempts:
            drug_id = random.choice(drug_ids)
            disease_id = random.choice(disease_ids)

            if (drug_id, disease_id) not in known_set:
                unknown_samples.append({
                    'drug_id': drug_id,
                    'disease_id': disease_id,
                    'drug_name': embeddings[drug_id]['name'],
                    'disease_name': embeddings[disease_id]['name'],
                    'label': 2,  # Unknown
                    'type': 'unknown'
                })

            attempts += 1

        print(f"  ✓ Unknown samples: {len(unknown_samples)}")

        # Combine all samples
        all_samples = positive_samples + negative_samples + unknown_samples
        random.shuffle(all_samples)

        print(f"\n  Total training samples: {len(all_samples)}")
        print(f"    - Positive (treats): {len(positive_samples)} ({len(positive_samples)/len(all_samples)*100:.1f}%)")
        print(f"    - Negative (doesn't treat): {len(negative_samples)} ({len(negative_samples)/len(all_samples)*100:.1f}%)")
        print(f"    - Unknown: {len(unknown_samples)} ({len(unknown_samples)/len(all_samples)*100:.1f}%)")

        return all_samples

    # =====================================================================
    # STEP 4: Flatten Embeddings into Feature Vectors
    # =====================================================================

    def create_feature_vectors(self, samples, embeddings):
        """
        Flatten drug + disease embeddings into feature vectors

        Input: Drug embedding [64] + Disease embedding [64]
        Output: Feature vector [128]
        """
        print("\n" + "="*80)
        print("STEP 4: Creating Feature Vectors")
        print("="*80)

        X = []  # Features
        y = []  # Labels
        valid_samples = []

        for sample in samples:
            drug_id = sample['drug_id']
            disease_id = sample['disease_id']

            if drug_id in embeddings and disease_id in embeddings:
                # Get embeddings
                drug_emb = embeddings[drug_id]['embedding']
                disease_emb = embeddings[disease_id]['embedding']

                # Concatenate: [drug_emb | disease_emb]
                feature_vector = np.concatenate([drug_emb, disease_emb])

                X.append(feature_vector)
                y.append(sample['label'])
                valid_samples.append(sample)

        X = np.array(X)
        y = np.array(y)

        print(f"✓ Feature matrix shape: {X.shape}")
        print(f"  - Samples: {X.shape[0]}")
        print(f"  - Features: {X.shape[1]} (drug_emb: {len(drug_emb)} + disease_emb: {len(disease_emb)})")

        return X, y, valid_samples

    # =====================================================================
    # STEP 5: Train/Test Split
    # =====================================================================

    def split_data(self, X, y, samples, test_size=0.2, random_state=42):
        """Split data into train and test sets"""
        print("\n" + "="*80)
        print("STEP 5: Splitting Data (Train/Test)")
        print("="*80)

        X_train, X_test, y_train, y_test, samples_train, samples_test = train_test_split(
            X, y, samples, test_size=test_size, random_state=random_state, stratify=y
        )

        print(f"✓ Training set: {len(X_train)} samples")
        print(f"  - Class 0 (doesn't treat): {sum(y_train == 0)}")
        print(f"  - Class 1 (treats): {sum(y_train == 1)}")
        print(f"  - Class 2 (unknown): {sum(y_train == 2)}")

        print(f"\n✓ Test set: {len(X_test)} samples")
        print(f"  - Class 0 (doesn't treat): {sum(y_test == 0)}")
        print(f"  - Class 1 (treats): {sum(y_test == 1)}")
        print(f"  - Class 2 (unknown): {sum(y_test == 2)}")

        return X_train, X_test, y_train, y_test, samples_train, samples_test

    # =====================================================================
    # STEP 6: Train XGBoost Model with Cross-Validation
    # =====================================================================

    def train_model(self, X_train, y_train, cv_folds=5):
        """Train XGBoost classifier with cross-validation"""
        print("\n" + "="*80)
        print("STEP 6: Training XGBoost Model")
        print("="*80)

        # Initialize XGBoost classifier
        model = XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            objective='multi:softprob',  # Multi-class classification
            num_class=3,  # 3 classes: 0, 1, 2
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )

        print("\nModel parameters:")
        print(f"  - n_estimators: {model.n_estimators}")
        print(f"  - max_depth: {model.max_depth}")
        print(f"  - learning_rate: {model.learning_rate}")

        # Cross-validation
        print(f"\n  Running {cv_folds}-fold cross-validation...")
        cv_scores = cross_val_score(
            model, X_train, y_train,
            cv=KFold(n_splits=cv_folds, shuffle=True, random_state=42),
            scoring='accuracy',
            n_jobs=-1
        )

        print(f"  ✓ Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        print(f"    Fold scores: {[f'{s:.4f}' for s in cv_scores]}")

        # Train on full training set
        print("\n  Training on full training set...")
        model.fit(X_train, y_train)
        print("  ✓ Training complete!")

        return model

    # =====================================================================
    # STEP 7: Evaluate Model on Test Set
    # =====================================================================

    def evaluate_model(self, model, X_test, y_test):
        """Evaluate model performance on test set"""
        print("\n" + "="*80)
        print("STEP 7: Evaluating Model on Test Set")
        print("="*80)

        # Predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)

        # Classification report
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred,
                                   target_names=['Does NOT Treat (0)', 'TREATS (1)', 'Unknown (2)']))

        # ROC-AUC for binary classification (class 1 vs rest)
        if len(np.unique(y_test)) > 1:
            y_test_binary = (y_test == 1).astype(int)
            y_pred_binary = y_pred_proba[:, 1]  # Probability of class 1 (treats)

            roc_auc = roc_auc_score(y_test_binary, y_pred_binary)
            print(f"\nROC-AUC (Class 1 'TREATS' vs rest): {roc_auc:.4f}")

            # Precision-Recall AUC
            precision, recall, _ = precision_recall_curve(y_test_binary, y_pred_binary)
            pr_auc = auc(recall, precision)
            print(f"Precision-Recall AUC: {pr_auc:.4f}")

        return y_pred, y_pred_proba

    # =====================================================================
    # STEP 8: Score ALL Drug-Disease Pairs
    # =====================================================================

    def score_all_pairs(self, drugs, diseases, known_pairs, embeddings, model):
        """Predict probabilities for ALL possible drug-disease pairs"""
        print("\n" + "="*80)
        print("STEP 8: Scoring ALL Drug-Disease Pairs")
        print("="*80)

        # Create set of known pairs
        known_set = {(p['drug_id'], p['disease_id']) for p in known_pairs}

        # Generate all possible pairs (excluding known treatments)
        all_pairs = []
        drug_ids = list(drugs.keys())
        disease_ids = list(diseases.keys())

        total_possible = len(drug_ids) * len(disease_ids)
        print(f"  Total possible pairs: {total_possible:,}")

        print("  Generating unknown pairs...")
        for drug_id in tqdm(drug_ids, desc="  Processing drugs"):
            for disease_id in disease_ids:
                # Skip known relationships (we already know these)
                if (drug_id, disease_id) not in known_set:
                    if drug_id in embeddings and disease_id in embeddings:
                        all_pairs.append({
                            'drug_id': drug_id,
                            'disease_id': disease_id,
                            'drug_name': embeddings[drug_id]['name'],
                            'disease_name': embeddings[disease_id]['name']
                        })

        print(f"  ✓ Unknown pairs to score: {len(all_pairs):,}")

        # Create feature vectors
        print("\n  Creating feature vectors...")
        X_all = []
        for pair in tqdm(all_pairs, desc="  Flattening embeddings"):
            drug_emb = embeddings[pair['drug_id']]['embedding']
            disease_emb = embeddings[pair['disease_id']]['embedding']
            feature_vector = np.concatenate([drug_emb, disease_emb])
            X_all.append(feature_vector)

        X_all = np.array(X_all)
        print(f"  ✓ Feature matrix: {X_all.shape}")

        # Predict probabilities
        print("\n  Predicting treatment probabilities...")
        predictions = model.predict_proba(X_all)

        # Add predictions to pairs
        for i, pair in enumerate(all_pairs):
            pair['prob_not_treat'] = predictions[i, 0]
            pair['prob_treats'] = predictions[i, 1]  # THIS IS THE KEY SCORE!
            pair['prob_unknown'] = predictions[i, 2]

        print("  ✓ Predictions complete!")

        return all_pairs

    # =====================================================================
    # STEP 9: Rank and Output Results
    # =====================================================================

    def rank_and_output(self, predictions, output_file='drug_discovery_predictions.csv'):
        """Rank predictions by treatment probability and output"""
        print("\n" + "="*80)
        print("STEP 9: Ranking and Outputting Results")
        print("="*80)

        # Convert to DataFrame
        df = pd.DataFrame(predictions)

        # Sort by probability of treatment (descending)
        df_sorted = df.sort_values('prob_treats', ascending=False).reset_index(drop=True)
        df_sorted['rank'] = df_sorted.index + 1

        # Add confidence level
        def confidence_level(score):
            if score >= 0.7:
                return 'HIGH'
            elif score >= 0.4:
                return 'MEDIUM'
            else:
                return 'LOW'

        df_sorted['confidence'] = df_sorted['prob_treats'].apply(confidence_level)

        # Reorder columns
        df_sorted = df_sorted[[
            'rank', 'drug_name', 'disease_name', 'prob_treats', 'confidence',
            'prob_not_treat', 'prob_unknown', 'drug_id', 'disease_id'
        ]]

        # Save to CSV
        df_sorted.to_csv(output_file, index=False)
        print(f"✓ Saved {len(df_sorted):,} predictions to: {output_file}")

        # Display top 20
        print("\n" + "="*80)
        print("TOP 20 DRUG REPURPOSING CANDIDATES")
        print("="*80)
        print(df_sorted.head(20).to_string(index=False))

        # Statistics
        print("\n" + "="*80)
        print("PREDICTION STATISTICS")
        print("="*80)
        print(f"Total predictions: {len(df_sorted):,}")
        print(f"\nConfidence distribution:")
        print(df_sorted['confidence'].value_counts())
        print(f"\nProbability statistics:")
        print(df_sorted['prob_treats'].describe())

        return df_sorted

    # =====================================================================
    # MAIN PIPELINE
    # =====================================================================

    def run_pipeline(self):
        """Run complete drug discovery pipeline"""
        print("\n" + "="*80)
        print("DRUG-DISEASE PREDICTION PIPELINE WITH XGBOOST")
        print("="*80)

        # Step 1: Load embeddings
        embeddings, drugs, diseases = self.load_embeddings()

        # Step 2: Load known relationships
        known_pairs = self.load_known_relationships()

        # Step 3: Create training data
        training_samples = self.create_training_data(
            drugs, diseases, known_pairs, embeddings,
            negative_ratio=1.0,
            unknown_ratio=0.5
        )

        # Step 4: Create feature vectors
        X, y, valid_samples = self.create_feature_vectors(training_samples, embeddings)

        # Step 5: Train/test split
        X_train, X_test, y_train, y_test, samples_train, samples_test = self.split_data(
            X, y, valid_samples, test_size=0.2
        )

        # Step 6: Train model
        model = self.train_model(X_train, y_train, cv_folds=5)

        # Step 7: Evaluate model
        y_pred, y_pred_proba = self.evaluate_model(model, X_test, y_test)

        # Step 8: Score all pairs
        all_predictions = self.score_all_pairs(drugs, diseases, known_pairs, embeddings, model)

        # Step 9: Rank and output
        results_df = self.rank_and_output(all_predictions,
                                         output_file='drug_discovery_predictions.csv')

        print("\n" + "="*80)
        print("PIPELINE COMPLETE!")
        print("="*80)
        print(f"✓ Results saved to: drug_discovery_predictions.csv")
        print(f"✓ Total predictions: {len(results_df):,}")
        print(f"✓ Top candidate: {results_df.iloc[0]['drug_name']} → {results_df.iloc[0]['disease_name']}")
        print(f"  (Probability: {results_df.iloc[0]['prob_treats']:.4f})")

        return model, results_df


def main():
    """Main entry point"""
    print("="*80)
    print("DRUG DISCOVERY WITH XGBOOST AND NODE2VEC EMBEDDINGS")
    print("="*80)
    print("\nPrerequisites:")
    print("  1. Memgraph running on bolt://localhost:7687")
    print("  2. Node embeddings generated (run: python generate_and_query_embeddings.py)")
    print("  3. Example data loaded (run: python load_example_progressive.py)")

    input("\nPress Enter to start the pipeline...")

    # Initialize predictor
    predictor = DrugDiseasePredictor(uri='bolt://localhost:7687')

    try:
        # Run complete pipeline
        model, results = predictor.run_pipeline()

        print("\n" + "="*80)
        print("NEXT STEPS")
        print("="*80)
        print("1. Review top predictions in: drug_discovery_predictions.csv")
        print("2. Investigate high-confidence candidates")
        print("3. Validate predictions with literature search")
        print("4. Design experiments for top candidates")

    finally:
        predictor.close()


if __name__ == "__main__":
    main()
