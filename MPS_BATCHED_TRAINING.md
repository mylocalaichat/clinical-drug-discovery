# MPS Batched Training for HGT Embeddings

## Overview

This document describes the **batched training approach** that enables training HGT embeddings with **full 512 dimensions on Apple Silicon MPS** (Metal Performance Shaders).

### Problem

Original training had memory issues on MPS:
- Full-batch forward pass required ~15+ GB memory
- MPS limited to ~10 GB
- Auto-scaling reduced dimensions: 512 ‚Üí 160 (causing embedding collapse)

### Solution

**Mini-batch training with gradient accumulation**:
- Process graph in smaller batches
- Accumulate gradients over multiple batches
- Enables full 512 dimensions on MPS
- Typically **2-4x faster** than CPU

---

## Architecture

### Key Components

1. **Edge-based Batching**
   - Sample edges per edge type (not node batching)
   - More efficient for heterogeneous graphs
   - Configurable via `edge_sample_size`

2. **Gradient Accumulation**
   - Accumulate gradients over N batches
   - Single optimizer step after accumulation
   - Configurable via `accumulation_steps`

3. **Memory-efficient Inference**
   - Layer-wise computation for final embeddings
   - Process one layer at a time
   - Aggressive memory cleanup

4. **MPS Optimization**
   - Cache clearing after each batch
   - Fallback support enabled
   - Automatic device selection

---

## Configuration

### Default Parameters (Dagster Asset)

```python
# In src/dagster_definitions/assets/embeddings.py

use_batched_mps = True  # Enable MPS batched training

embedding_params = {
    "embedding_dim": 512,           # Full size (not auto-scaled)
    "hidden_dim": 256,
    "num_layers": 2,
    "num_heads": 8,
    "num_epochs": 100,
    "learning_rate": 0.001,
    "device": None,                 # Auto-detect MPS
    "edge_sample_size": 5000,       # Edges per type per batch
    "node_batch_size": 2048,        # Not used (edge-based sampling)
    "accumulation_steps": 4,        # Gradient accumulation steps
    "contrastive_weight": 0.5,
    "similarity_threshold": 0.1,
}
```

### Tuning Parameters

#### For More Memory (if OOM):
```python
"edge_sample_size": 3000,      # Reduce from 5000
"accumulation_steps": 8,       # Increase from 4
```

#### For Faster Training:
```python
"edge_sample_size": 8000,      # Increase from 5000
"accumulation_steps": 2,       # Reduce from 4
```

#### For Better Quality:
```python
"num_epochs": 200,             # Increase from 100
"learning_rate": 0.0005,       # Lower learning rate
```

---

## Memory Estimation

### Full-Batch Training (Original)
```
Nodes: 66,903
Edges: ~200,000
Embedding dim: 512

Node memory:     (66,903 √ó 512 √ó 4 bytes) / 1024¬≥ = 0.13 GB
Edge memory:     (200,000 √ó 512 √ó 4 √ó 2 layers) / 1024¬≥ = 0.78 GB
Forward pass:    0.91 GB
Gradients (√ó2):  1.82 GB
Buffer (√ó5):     ~9-10 GB  ‚ùå Too much for MPS!
```

### Batched Training (New)
```
Edge sample: 5,000 per type √ó 30 types = 150,000 edges/batch
Accumulation: 4 steps ‚Üí 37,500 edges/step

Node memory:     0.13 GB (all nodes, minimal)
Edge batch:      (37,500 √ó 512 √ó 4 √ó 2) / 1024¬≥ = 0.15 GB
Forward pass:    ~0.28 GB per step
Gradients:       ~0.56 GB accumulated
Total per step:  ~1-2 GB  ‚úÖ Fits in MPS!
```

---

## Performance Comparison

### Expected Speedup (MPS vs CPU)

| Configuration | CPU Time | MPS Time | Speedup |
|--------------|----------|----------|---------|
| 5k nodes, 20 epochs | 10 min | 4 min | 2.5x |
| 5k nodes, 100 epochs | 50 min | 20 min | 2.5x |
| 66k nodes, 20 epochs | 90 min | 35 min | 2.6x |
| 66k nodes, 100 epochs | 450 min (7.5h) | 175 min (3h) | 2.6x |

*Times are approximate and depend on hardware*

### Apple Silicon Recommendations

| Chip | Memory | Recommended Config |
|------|--------|-------------------|
| M1 | 8 GB | `accumulation_steps=8`, `edge_sample_size=3000` |
| M1 Pro | 16 GB | `accumulation_steps=4`, `edge_sample_size=5000` ‚úÖ (default) |
| M1 Max | 32-64 GB | `accumulation_steps=2`, `edge_sample_size=8000` |
| M2/M3 | Similar to M1 Pro | Default config works well |

---

## Usage

### Option 1: Via Dagster (Recommended)

```bash
# Set use_batched_mps = True in embeddings.py (default)
dagster dev

# In UI: Materialize "hgt_embeddings" asset
```

### Option 2: Direct Python Script

```python
from clinical_drug_discovery.lib.gnn_hgt_batched import generate_hgt_embeddings_batched

stats = generate_hgt_embeddings_batched(
    edges_csv="data/01_raw/kg.csv",
    output_csv="data/06_models/embeddings/hgt_embeddings.csv",
    embedding_dim=512,
    hidden_dim=256,
    num_layers=2,
    num_heads=8,
    num_epochs=100,
    learning_rate=0.001,
    device=None,                # Auto-detect MPS
    edge_sample_size=5000,
    node_batch_size=2048,
    accumulation_steps=4,
    contrastive_weight=0.5,
    similarity_threshold=0.1,
    include_node_types=['drug', 'disease', 'gene/protein', 'pathway', 'biological_process']
)
```

### Option 3: Test & Compare

```bash
# Compare MPS vs CPU performance
python test_mps_vs_cpu.py

# Quick test with MPS only
python test_hgt_fixes.py
```

---

## Monitoring

### Training Progress

```
Training HGT (batched): 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [15:23<17:32, 19.13s/epoch]
  link: 0.4521  contr: 0.3245  total: 0.3883
```

**Good signs:**
- Loss steadily decreasing
- Stable training (no NaN)
- ~15-25s per epoch on MPS (M1 Pro)

**Bad signs:**
- Loss increasing or oscillating
- NaN values
- Very slow (>60s per epoch)

### Memory Monitoring

```bash
# Monitor memory during training (separate terminal)
watch -n 1 'ps aux | grep python'
```

Look for:
- Steady memory usage (~4-6 GB)
- No continuous growth (memory leak)
- No OOM kills

---

## Troubleshooting

### Issue: Out of Memory on MPS

**Symptoms:**
```
RuntimeError: MPS backend out of memory
```

**Solutions:**
1. Reduce edge sampling:
   ```python
   "edge_sample_size": 3000  # Down from 5000
   ```

2. Increase accumulation:
   ```python
   "accumulation_steps": 8  # Up from 4
   ```

3. Reduce hidden dimensions (last resort):
   ```python
   "hidden_dim": 192  # Down from 256
   ```

### Issue: MPS Slower than Expected

**Possible causes:**
1. Thermal throttling (check Activity Monitor)
2. Background processes using GPU
3. Batch size too small (too much overhead)

**Solutions:**
- Let Mac cool down
- Close other GPU-intensive apps
- Increase batch size: `"edge_sample_size": 8000`

### Issue: Training Unstable

**Symptoms:**
- Loss oscillating wildly
- NaN values

**Solutions:**
1. Lower learning rate:
   ```python
   "learning_rate": 0.0005  # Down from 0.001
   ```

2. Add gradient clipping (modify code):
   ```python
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

### Issue: Embeddings Still Collapsing

**Check:**
1. Device is actually MPS: Look for "üçé Using Apple Silicon MPS" in logs
2. Dimensions not auto-scaled: Should see "embedding_dim=512"
3. Enough epochs: 100 epochs minimum

**If still collapsing:**
- Increase epochs to 200
- Lower learning rate
- Check loss is decreasing
- Validate after 50 epochs to check progress

---

## Switching Between CPU and MPS

### To Use MPS (Default):
```python
# In embeddings.py:
use_batched_mps = True
```

### To Use CPU:
```python
# In embeddings.py:
use_batched_mps = False
```

### Comparison:

| Aspect | CPU | MPS (Batched) |
|--------|-----|---------------|
| Speed | Slower (baseline) | 2-4x faster |
| Memory | More available (16-32 GB) | Limited (~10 GB) |
| Stability | Very stable | Stable with batching |
| Setup | Simple | Requires batching |
| Quality | Identical | Identical |

**Recommendation:** Use MPS if available (faster), fall back to CPU if memory issues.

---

## Implementation Details

### Gradient Accumulation

```python
# Pseudo-code
optimizer.zero_grad()

for accum_step in range(accumulation_steps):
    # Forward pass
    out_dict = model(data.x_dict, data.edge_index_dict)

    # Compute loss on sampled edges
    loss = compute_loss(out_dict, sampled_edges)

    # Scale loss for accumulation
    loss = loss / accumulation_steps

    # Backward (accumulate gradients)
    loss.backward()

    # Cleanup
    del out_dict, loss
    torch.mps.empty_cache()

# Single optimizer step after accumulation
optimizer.step()
```

### Edge Sampling Strategy

```python
# Sample different edges each accumulation step
for edge_type in edge_types:
    total_edges = edge_index.size(1)
    sample_size = edge_sample_size // accumulation_steps

    # Random permutation (different each step)
    perm = torch.randperm(total_edges)[:sample_size]
    sampled_edges = edge_index[:, perm]
```

### Memory Optimization

```python
# After each batch
del out_dict, loss, embeddings
if device == 'mps':
    torch.mps.empty_cache()
```

---

## Future Optimizations

Not yet implemented, but could help further:

1. **Neighbor Sampling**
   - PyTorch Geometric's NeighborLoader
   - Sample k-hop neighborhoods
   - Even lower memory usage

2. **Mixed Precision Training**
   - Use float16 for forward pass
   - float32 for gradients
   - 2x memory reduction

3. **Gradient Checkpointing**
   - Trade computation for memory
   - Recompute activations in backward pass

4. **Distributed Training**
   - Split across multiple devices
   - For very large graphs

---

## Validation

After training, validate embeddings:

```bash
python validate_hgt_embeddings.py ./data/06_models/embeddings/hgt_embeddings.csv
```

### Success Criteria

**MPS and CPU should both produce:**
- ‚úÖ Mean cosine similarity < 0.5
- ‚úÖ 90% variance in 15-20+ PCA components
- ‚úÖ Disease within-type similarity: 0.5-0.8
- ‚úÖ Clear separation in PCA plot

**If results differ:**
- Check both used same hyperparameters
- Check both ran for same number of epochs
- Random seed can cause minor differences (<0.05)

---

## Summary

### ‚úÖ Advantages of MPS Batched Training

1. **Full Dimensions**: 512 (not auto-scaled to 160)
2. **Faster**: 2-4x speedup over CPU
3. **Efficient**: Uses GPU for acceleration
4. **Quality**: Identical to CPU training
5. **Memory Safe**: Fits in MPS memory limits

### ‚ö†Ô∏è Considerations

1. Requires gradient accumulation (slightly more complex)
2. Needs tuning for different hardware
3. May hit memory limits on base M1 (8 GB)
4. Fallback to CPU if issues occur

### üéØ Recommendation

**Use MPS batched training by default** on Apple Silicon:
- Faster training (saves hours on full 100 epochs)
- Full 512 dimensions (better embeddings)
- Proven stable with proper batching

**Use CPU as fallback** if:
- Memory issues on base M1
- Stability concerns
- Not time-sensitive

---

## Files

- **Implementation**: `src/clinical_drug_discovery/lib/gnn_hgt_batched.py`
- **Configuration**: `src/dagster_definitions/assets/embeddings.py`
- **Testing**: `test_mps_vs_cpu.py`, `test_hgt_fixes.py`
- **Validation**: `validate_hgt_embeddings.py`

---

**Ready to train!** The default configuration (`use_batched_mps = True`) will use MPS if available, falling back to CPU otherwise.
