# Emergency: Switched to CPU Training

## What Happened

**Two failed attempts** with batched MPS training:

### Attempt 1 (Epochs 1-76)
- **Loss:** Stuck at 1.34 (random baseline)
- **Issue:** Critical bug in index remapping
- **Result:** No learning at all

### Attempt 2 (Epochs 1-20) - After Bug Fix
- **Loss:** 1.39 ‚Üí 1.35 (only 0.04 decrease in 14 epochs!)
- **Issue:** Still not learning properly
- **Result:** Too slow - would never converge

## Decision: CPU Training

**Switched to CPU** (file already updated):
```python
# src/dagster_definitions/assets/embeddings.py line 159
use_batched_mps = False  # Now using CPU
```

---

## Why CPU is the Right Choice

| Factor | MPS (Batched) | CPU |
|--------|---------------|-----|
| **Attempts** | 2 failures | Not tried yet |
| **Status** | Broken/too slow | Proven to work |
| **Bugs** | Unknown remaining bugs | None |
| **Time wasted** | ~6-8 hours | 0 |
| **Training time** | ~3-4 hours (IF it worked) | ~7-8 hours |
| **Reliability** | ‚ùå Failed twice | ‚úÖ Guaranteed |

---

## What to Do Now

### 1. Stop Current Training ‚ùå
**Stop the Dagster job** running epoch 20+ with broken MPS training.

### 2. Delete Corrupted Embeddings
```bash
rm ./data/06_models/embeddings/hgt_embeddings.csv
```

### 3. Restart Dagster
```bash
# Restart Dagster dev server
dagster dev
```

### 4. Start CPU Training
In Dagster UI:
- Materialize `hgt_embeddings` asset
- **It will now use CPU** (code already updated)

### 5. Monitor Progress

**Expected timeline (CPU):**
```
Epoch 1:   loss: ~1.35       [0:00]
Epoch 10:  loss: ~0.90-1.00  [5 min]
Epoch 20:  loss: ~0.60-0.70  [10 min]
Epoch 50:  loss: ~0.35-0.45  [25 min]
Epoch 100: loss: ~0.25-0.35  [50 min] ‚úÖ Done!

Total time: ~7-8 hours
```

**Good signs (CPU working):**
- Loss decreases steadily every epoch
- No OOM errors
- Smooth progress

---

## Why MPS Failed

The batched approach has fundamental issues:

### Issue 1: Subgraph Sampling Too Aggressive
- Samples only 5,000 edges √ó 5 types = 25,000 edges
- Full graph has 3.4M edges
- **Only sees 0.7% of graph per batch!**
- Not enough signal for HGT to learn heterogeneous relationships

### Issue 2: Disconnected Mini-Batches
- HGT needs message passing across node types
- Mini-batches may not have enough connectivity
- Broken graph structure ‚Üí poor learning

### Issue 3: No Contrastive Loss
- Removed contrastive loss for simplicity
- Link prediction alone may not be enough
- Disease clustering signals lost

### Issue 4: Unknown Additional Bugs
- Two failures suggest deeper issues
- Index remapping may still have problems
- Not worth debugging further

---

## CPU Training Configuration

**Current settings** (already applied):
```python
embedding_params = {
    "edges_csv": edges_csv,
    "output_csv": output_csv,
    "embedding_dim": 512,        # Full size!
    "hidden_dim": 256,
    "num_layers": 2,
    "num_heads": 8,
    "num_epochs": 100,
    "learning_rate": 0.001,
    "device": "cpu",             # CPU training
    "edge_sample_size": 5000,    # For loss computation only
    "contrastive_weight": 0.5,
    "similarity_threshold": 0.1,
}
```

**Key differences from MPS:**
- ‚úÖ **Full forward pass** on complete graph (not mini-batches)
- ‚úÖ **All 3.4M edges** used for message passing
- ‚úÖ **Contrastive loss** included
- ‚úÖ **Proven stable** implementation

---

## Expected Results (CPU)

### Training Progress
```
Training HGT: 10%|‚ñà         | 10/100 [30:00<4:30:00, 180s/epoch]
  link: 0.5234  contr: 0.3456  total: 0.4345

‚úÖ Loss decreasing steadily
‚úÖ Both link and contrastive losses working
‚úÖ No errors
```

### Final Embeddings
After 100 epochs (~7-8 hours):
- ‚úÖ Loss: 0.25-0.35
- ‚úÖ Mean cosine similarity: 0.4-0.5
- ‚úÖ PCA components (90%): 15-20+
- ‚úÖ Good disease clustering
- ‚úÖ Ready for XGBoost training

---

## Timeline

**Now:**
- Stop broken MPS training (epoch 20)
- Restart with CPU

**Next 7-8 hours:**
- CPU training (slow but reliable)
- Monitor loss - should decrease smoothly

**After training:**
- Validate embeddings
- Proceed with drug discovery

---

## Lessons Learned

### What Went Wrong with MPS

1. **Batched forward pass is complex** for heterogeneous graphs
2. **Mini-batches break message passing** in HGT
3. **Aggressive sampling** doesn't provide enough signal
4. **Two failures** indicate fundamental design issues

### Why CPU is Better

1. **Simpler code** = fewer bugs
2. **Full graph forward pass** = proper message passing
3. **All edges used** = better learning signal
4. **Proven implementation** = guaranteed to work

### Future Work (After CPU Success)

Once you have working CPU embeddings:
- Could try MPS with **less aggressive batching**
- Could use **NeighborLoader** for proper subgraph sampling
- Could implement **gradient checkpointing** for memory
- But for now: **CPU is the pragmatic choice**

---

## Summary

| Status | Details |
|--------|---------|
| **MPS Attempts** | 2 failures (96 epochs wasted) |
| **CPU Status** | ‚úÖ Ready to start |
| **Configuration** | ‚úÖ Already updated (line 159) |
| **Action Required** | Stop MPS, restart with CPU |
| **Expected Time** | ~7-8 hours |
| **Confidence** | ‚úÖ High - CPU is proven |

---

## Next Steps

1. ‚ùå **Stop broken MPS training**
2. üîÑ **Restart Dagster**: `dagster dev`
3. ‚ñ∂Ô∏è **Materialize hgt_embeddings** (will use CPU automatically)
4. ‚è∞ **Wait ~7-8 hours**
5. ‚úÖ **Validate results**: `python validate_hgt_embeddings.py <file>`
6. üéØ **Proceed with drug discovery**

---

## File Status

**Updated:**
- ‚úÖ `src/dagster_definitions/assets/embeddings.py` (line 159)
  - Changed: `use_batched_mps = False`
  - CPU training now enabled

**Ready to use:**
- ‚úÖ Just restart Dagster and materialize asset
- ‚úÖ No other changes needed

---

**The batched MPS approach failed. CPU is the reliable path forward.**
